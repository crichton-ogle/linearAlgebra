\documentclass{ximera}
\input{../preamble.tex}
\title{Matrices}
\author{Crichton Ogle}

\begin{document}
\begin{abstract}
  A matrix is a rectangular array whose entries are of the same type.
\end{abstract}
\maketitle

We begin by recalling some notation and terminology regarding matrices. A {\it matrix} will mean a rectangular array whose entries are of the same type. Thus, we could have an array of real numbers, complex numbers, functions, or even matrices. An {\it $m\times n$ matrix} will refer to one which has $m$ rows and $n$ columns, and the {\it collection of all $m\times n$ matrices of real numbers} will be denoted by $\mathbb R^{m\times n}$. We adopt the convention, used by MATLAB, in which the $(i,j)^{th}\ entry$ of the matrix $A$ (that in row $i$ and column $j$) is denoted by $A(i,j)$. Also, following MATLAB notation, we will write the $i^{th}$ row as $A(i,:)$, and the $j^{th}$ column as $A(:,j)$. Before getting to the operations themselves, we first record

\begin{definition} Two matrices $A$ and $B$ are {\it equal} if $A(i,j) = B(i,j)$ for all $i,j$.
\end{definition}
Note that this equality forces $A$ and $B$ to have the same dimensions, because if they had different dimensions, there would have to be a choice of indices $(i,j)$ for which one side of the equation exists, but the other does not. Thus, equality can be reformulated as saying: $A$ and $B$ have the same dimensions, and the same entry in each place.
\vskip.2in
Matrix algebra uses three different types of operations.
\vskip.1in

{\bf\underbar{Matrix Addition}} If $A$ and $B$ have the same dimensions, then the sum $A+B$ is given by\footnote{ In what follows, the symbol \lq\lq$:=$\rq\rq\ is used to indicate that the expression to the left of the symbol is {\it defined to be} what appears to the right of the symbol.}
\[
(A+B)(i,j) := A(i,j) + B(i,j)
\]

Note that the dimensions of $A+B$ are the same as those of (both) $A$ and $B$.
\vskip.1in

{\bf\underbar{Scalar Multiplication}} If $A$ is a matrix and $\alpha$ a scalar, the {scalar product} of $\alpha$ with $A$ is given by
\[
(\alpha A)(i,j) := (\alpha)A(i,j)
\]
There are no restrictions on the dimension of $A$ for this operation to be defined.
\vskip.1in

{\bf\underbar{Matrix Multiplication}} This is the most complicated of the three operations. If $A$ is $m\times n$ and $B$ is $p\times q$, then in order for the product $A*B$ to be defined, we require that $n = p$; this condition is expressed by saying that {\it the internal dimensions agree}. In this case
\[
(A*B)(i,k) := \sum_{j=1}^n A(i,j)B(j,k)
\]
The dimensions of the product are $m\times q$.
\vskip.1in

These different types of products may both be viewed as extensions of ordinary multiplication of real numbers. In fact, if we identify numerical $1\times 1$ matrices with scalars, then in that dimension the two product operations both correspond to ordinary multiplication. It is important to note that matrix multiplication can be performed whenever the sum of products appearing on the right-hand side is well-defined; in other words, for more than just numerical matrices. This fact will come into play later on.
\vskip.2in

The operations for matrix algebra satisfy similar properties to those for addition and multiplication of real numbers. The following theorem lists those properties. In each case, the expression on the left is defined iff that on the right is also defined.

\begin{theorem}\label{thm:matalg} Let $A,B,C$ denote matrices, and $\alpha,\beta$ scalars.

\begin{enumerate}
\item $A+B = B+A$ (commutativity of addition);
\item $A+(B+C) = (A+B)+C$ (associativity of addition);
\item $\alpha(A+B) = \alpha A + \alpha B$ (scalar multiplication distributes over matrix addition);
\item $A*(B+C) = A*B + A*C$ (matrix multiplication left-distributes over matrix addition);
\item $(A+B)*C = A*C + B*C$ (matrix multiplication right-distributes over matrix addition);
\item $(\alpha\beta)A = \alpha(\beta A)$ (associativity of scalar multiplication);
\item $A*(B*C) = (A*B)*C$ (associativity of matrix multiplication).
\end{enumerate}
\end{theorem}

This theorem is proven by showing that, in each case, the matrix on the left has the same $(i,j)^{th}$ entry as the one on the right. However, as with any proof, one needs to be clear from the beginning exactly what one is allowed to {\it assume} as being true. In this case, we have i) the definition of what it means for two matrices to be equal, ii) the explicit definition of each operation, and iii) the corresponding properties for addition and multiplication for real numbers (which will be taken as axioms for this proof). To see how this works, let's verify the first equality. 

\begin{proof} (of 3.2.1)
\begin{align*}
(A+B)(i,j) &= A(i,j) + B(i,j)\qquad{\rm by\ the\ definition\ of\ matrix\ addition}\\
                &= B(i,j) + A(i,j)\qquad{\rm by\ commutativity\ of\ addition\ for\ real\ numbers}\\
               &= (B+A)(i,j)\qquad\quad\ {\rm by\ the\ definition\ of\ matrix\ addition}
\end{align*}
\end{proof}

Notice that the proof consists of a sequence of equalities, beginning with the left-hand side of the equation we wish to verify, and ending with the right-hand side of that equation. Moreover, each equality in the sequence is justified by either a definition, or an axiom. Not all of the equalities are that easy; some may require more steps. To illustrate a more involved proof, we will verify property 7 (probably the most difficult to prove of the properties listed).

\begin{proof} (of 3.2.7) In this proof, $i,j,k,l$ will be used as indices (the reason for using four different indices will become apparent).
\begin{alignat*}{3}
(A*(B*C))(i,l)  =& \sum_j A(i,j)(B*C)(j,l)&&\qquad{\rm by\ the\ definition\ of\ matrix\ multiplication}&\\
                =& \sum_j A(i,j)\left(\sum_k B(j,k)C(k,l)\right)&&\qquad{\rm by\ the\ definition\ of\ matrix\ multiplication}&\\
                =& \sum_j\left(\sum_k A(i,j)\Big(B(j,k)C(k,l)\Big)\right)&&\qquad{\rm by\ property\ 4\ for\ real\ numbers}&\\
                =& \sum_j\left(\sum_k \Big(A(i,j)(B(j,k)\Big)C(k,l)\right)&&\qquad{\rm by\ property\ 7\ for\ real\ numbers}&\\
                =& \sum_k\left(\sum_j \Big(A(i,j)(B(j,k)\Big)C(k,l)\right)&&\qquad{\rm by\ property\ 1\ for\ real\ numbers}&\\
                =& \sum_k\left(\Big(\sum_j A(i,j)(B(j,k)\Big)C(k,l)\right)&&\qquad{\rm by\ property\ 5\ for\ real\ numbers}&\\
                =& \sum_k\left(\sum_k (A*B)(i,k)C(k,l)\right)&&\qquad{\rm by\ the\ definition\ of\ matrix\ multiplication}&\\
                =& ((A*B)*C)(i,l)&&\qquad{\rm by\ the\ definition\ of\ matrix\ multiplication}&\\
\end{alignat*}
\end{proof}
\vskip.2in

\begin{exercise} Using the above two proofs as models, prove properties (3.2.2) - (3.2.6).
\end{exercise}

Before moving on to considering equations, we introduce a few more matrix operations.

\begin{definition} The {\it transpose} of the matrix $A$, written as $A^T$, is always defined, and given by
\[
\left(A^T\right)(i,j) := A(j,i)
\]
\end{definition}
The way this operation relates to the algebraic operations defined above is described by the next theorem.

\begin{theorem} Let $A$ and $B$ be matrices. Then
\begin{enumerate}
\item $(A+B)^T = A^T + B^T$;
\item $(A*B)^T = B^T*A^T$;
\item $\left(A^T\right)^T = A$.
\end{enumerate}
\end{theorem}

\begin{exercise} Verify these properties in the same manner as in the previous exercise.
\end{exercise}

Also, one can concatenate matrices. Specifically,

\begin{definition} If $A$ is $m\times n$ and $B$ is $m\times p$, then the {\it horizontal} concatenation of $A$ and $B$ is written as
\[
\begin{bmatrix} 
A & B
\end{bmatrix};
\] 
it is the $m\times (n+p)$ matrix where $A$ appears as the left-most $m\times n$ block, and $B$ appears as the right-most $m\times p$ block. Similarly, if $C$ is $q\times n$, then the {\it vertical concatenation} of $A$ and $C$ is written as
\[
\begin{bmatrix} 
A\\ C
\end{bmatrix};
\]
It is the $(m+q)\times n$ matrix where $A$ appears in the upper $m\times n$ block, and $C$ in the lower $q\times n$ block.
\end{definition}
Concatenation can be done multiple times. Any matrix $A$ can be viewed as 
\begin{itemize}
\item the horizontal concatenation of its columns, and
\item the vertical concatenation of its rows.
\end{itemize}

In what follows, the horizontal type of concatenation will be used much more often than vertical one; for that reason {\it concatenation} (direction unspecified) will refer to {\it horizontal concatenation}. The following exercise will allow the reader to better understand how concatenation interacts with the algebraic operations, and the transpose. It is not an exhaustive list.

\begin{exercise} Let $A, B, C, D$ denote matrices all with the same number of rows. Show that
\begin{enumerate}
\item If the pairs $A,C$ and $B,D$ the same number of columns as well, then
\[
\begin{bmatrix} A & B\end{bmatrix} + \begin{bmatrix} C & D\end{bmatrix} = \begin{bmatrix} (A+C) & (B+D)\end{bmatrix}.
\]
\item With respect to products, one has
\[
A*\begin{bmatrix} B & C\end{bmatrix} = \begin{bmatrix} A*B & A*C\end{bmatrix}.
\]
\item With respect to transpose, one has
\[
\begin{bmatrix} A & B\end{bmatrix}^T = \begin{bmatrix} A^T \\ B^T\end{bmatrix}
\]
\end{enumerate}
\end{exercise}

Finally, we discuss the identity matrix and inverses. Recall that a {\it number} $\alpha$ is invertible if there is another number $\beta$ such that $\alpha\beta = 1$. A similar notion exists for matrices. To explain it, we first need to define the matrix equivalent of the number \lq\lq 1".

\begin{definition} The {\it identity matrix} $I = I^{n\times n}$ is the $n\times n$ matrix with $I(i,j) = \begin{cases}1\ \ \rm{if}\ i=j\\0\ \ \rm{if}\ i\ne j\end{cases}$.
\end{definition}

In most cases the dimension of $I$ will not be indicated, as it will be uniquely determined by the manner in which it is being used. For example, if it appears as a term in a matrix product, then its dimension is assumed to be the one which makes the product well-defined. This rule applies for the following 

\begin{proposition} For any matrix $A$, $I*A = A$ and $A*I = A$
\end{proposition}

\begin{exercise} Verify these two equalities.
\end{exercise}

\begin{definition} An $m\times n$ matrix $A$ is {\it invertible} if there is an $n\times m$ matrix $B$ satisfying
\[
A*B = I^{m\times m}, B*A = I^{n\times n}
\]
\end{definition}

Apriori, is seems there is no dimensional restriction on a matrix for it to be invertible. However, the following theorem clarifies the situation. The reason for why it is true will become clear later on when we discuss the rank of a matrix.

\begin{theorem} A matrix $A$ can only be invertible if it is square ($m = n$). In this case $A*B =I$ iff $B*A = I$ (every one-sided inverse is a two-sided inverse).
\end{theorem}

\begin{exercise} Show that the inverse of an invertible matrix is unique.
\end{exercise}

Given this, we will refer to {\it the} inverse of an invertible matrix $A$, and write it as $A^{-1}$. An alternative term for invertible is {\it non-singular} (so {\it singular} is equivalent to being  {\it non-invertible}). Two important questions are: under what conditions is a square matrix non-singular? And if a matrix is non-singular, how can one find its inverse? These questions are answered by 

\begin{theorem} Let $A$ be an $n\times n$ matrix. Then either
\begin{itemize}
\item $rref(A) = I$, which happens precisely when $A$ is non-singular;
\item the bottom row of $rref(A)$ is entirely zero, which happens precisely when $A$ is singular.
\end{itemize}
Moreover, when $A$ is non-singular, one has
\[
rref([A\ \ I]) = [I\ \ A^{-1}]
\]
\end{theorem}

\begin{exercise} Using the above theorem, show that the only matrix which is both invertible and in reduced row echelon form is the identity matrix (of a given dimension).

\end{exercise}


\end{document}
